{
    "contents" : "library(polynom)\nlibrary(manipulate)\n\n#q is degree of polynomial, n is number of examples, s is sigma squared\n\noverfit.plot <- function(q,n,s) {\n    #set.seed(127)\n    x <- runif(n, min = -1, max = 1)\n    \n    epsi <- rnorm(n) #noise\n    poly <- polynomial(rnorm(n = q+1)) #poly\n    y <- predict(poly, x) + sqrt(s)*epsi #values of poly +noise\n    \n    df <- data.frame(x, y)\n    \n    model_2 <- lm(y ~ poly(x,2), data = df)\n    model_10 <- lm(y ~ poly(x, 10), data = df)\n    \n    \n    test <- runif(2000, min = -1, max = 1)\n    testdf <- data.frame(test)\n    colnames(testdf)[1]<-\"x\"\n    newy_2 <- predict(model_2, newdata = testdf)\n    newy_10 <- predict(model_10, newdata = testdf)\n   \n    plot(test, predict(poly,test), ylim = c(-3,3), xlab = \"x\", ylab = \"y\", pch = 20 )\n    points(test, newy_10, col = \"red\", pch = 20)\n    points(test, newy_2, col = \"green\", pch = 20)\n    points(x, y, col = \"dark blue\")\n}\n\nmanipulate(overfit.plot(q, n, s),\n           q = slider(min = 1, max = 20),\n           n = slider(min = 20, max = 200),\n           s = slider(min = .01, max = 2))\n\n\n\nEout_2 <- sum\nEout_2 <- sum((predict(poly,test)-newy_2)^2)/1000\nEout_10 <- sum((predict(poly,test)-newy_10)^2)/1000\n\n\nOverfit <- function(q, n, s) {\n\n    x <- runif(n, min = -1, max = 1)\n    \n    poly <- polynomial(rnorm(n = q+1)) #poly\n    \n    epsi <- rnorm(n) #noise\n    \n    y <- predict(poly, x) + sqrt(s)*epsi #values of poly +noise\n    df <- data.frame(x, y)\n    \n    model_2 <- lm(y ~ poly(x,2), data = df)\n    model_10 <- lm(y ~ poly(x, 10), data = df)\n    \n    \n    #now we want E_out\n    test <- runif(1000, min = -1, max = 1)\n    testdf <- data.frame(test)\n    colnames(testdf)[1]<-\"x\"\n    newy_2 <- predict(model_2, newdata = testdf)\n    newy_10 <- predict(model_10, newdata = testdf)\n    \n    Eout_2 <- sum((predict(poly,test)-newy_2)^2)/1000\n    \n    Eout_10 <- sum((predict(poly,test)-newy_10)^2)/1000\n    \n    return(c(Eout_2, Eout_10, Eout_10 - Eout_2))\n}\n\n\nmean(replicate(500,Overfit(20, 40, 1)[3]))\nmedian(replicate(500,Overfit(20, 40, 1)[3]))\nhist(replicate(500,Overfit(20, 40, 1)[3])) #VERY skewed\nrange(replicate(500,Overfit(20, 40, 1)[3])) # most are close but overfitting is REALLY terrible \n#on a few occasions - this brings the MSE up but not the median.\n\nQ <- c(5, 10, 15, 20, 25, 30)\n\nN<- c(40, 60, 80, 100, 120, 140, 160)\n\nsigma <- c(0, 0.25, 0.5, 0.75, 1, 1.25, 1.5, 1.75, 2)\n\n#Let's vary N and keep Q = 10, Sigma = .5\ntrainexamples <- numeric(length(N))\nj = 1\nfor (i in N) { \n    trainexamples[j] <- mean(replicate(500,Overfit(10, i, .5)[3]))\n    j = j+1\n}\n\nplot( N[2:7] , trainexamples[2:7])\n\n\n#Let's vary Noise: Keep N = 80, sigma = .5\n\ntrainnoise = numeric(length(sigma))\nj = 1\nfor (i in sigma) { \n    trainnoise[j] <- mean(replicate(500,Overfit(10, 80, i)[3]))\n    j = j+1\n}\n\nplot(sigma, trainnoise)\n\n#Let's do complexity\n\ntraincomplexity = numeric(length(Q))\nj = 1\nfor (i in Q) { \n    traincomplexity[j] <- mean(replicate(500,Overfit(i, 80, 1)[3]))\n    j = j+1\n}\n\nplot(Q, traincomplexity, abline = 0)\n\nabline(h = 0, v = 0)\n\n?plot\n\n#____________________________________\n\n\nhist(replicate(500,Overfit(20, 40, 1)[1]))\nhist(replicate(500,Overfit(20, 80, 1)[3]))\n\n\nset.seed(124)\nmean(replicate(500,Overfit(10, 80, 1)[3]))\nmedian(replicate(500,Overfit(10, 80, 1)[3]))\n\n",
    "created" : 1456098235934.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "3939405680",
    "id" : "5CB81660",
    "lastKnownWriteTime" : 1456102717,
    "path" : "~/Documents/github/legendre_overfitting/Polynomial_Overfitting/Legendre_Overfitting.R",
    "project_path" : null,
    "properties" : {
    },
    "relative_order" : 5,
    "source_on_save" : false,
    "type" : "r_source"
}